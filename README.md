# ğŸŒ¸ SE-ResNet PyTorch Implementation

This repository contains a PyTorch implementation of **SE-ResNet**, integrating **Squeeze-and-Excitation (SE) blocks** into **ResNet bottleneck and basic blocks**. The model is designed for **enhanced channel-wise feature recalibration**, improving representational power while keeping computation efficient.  

- Implemented **SE-ResNet** with **residual blocks** and **SE modules**.  
- Architecture:  
**Stem â†’ Residual Blocks + SE â†’ GlobalAvgPool â†’ Flatten â†’ FC**

> **Note on SE blocks:** Each SE module performs **squeeze (global pooling) â†’ excitation (channel-wise scaling)**, allowing the network to emphasize informative features 
$$F_c(x)$$
dynamically.

**Paper reference:** [Squeeze-and-Excitation Networks (SE-ResNet)](https://arxiv.org/abs/1709.01507) ğŸ€

---

## ğŸ–¼ Overview â€“ SE-ResNet Architecture

![Figure 1](images/figures.jpg)  
*FigureÂ 1:* A Squeezeâ€‘andâ€‘Excitation (SE) block â€” shows the Squeeze + Excitation + Scale pipeline: global spatial info is pooled into a channel descriptor, then channelâ€‘wise activations are learned and used to recalibrate feature maps. 

*FigureÂ 2:* The schema comparing (left) a standard module (e.g. Inception or residual branch) and (right) its SEâ€‘augmented version â€” illustrating how the SE block wraps around the transformation to add channelâ€‘wise recalibration without disturbing spatial operations.

*FigureÂ 3:* The SEâ€‘ResNet module â€” shows how in a residual network the SE block is applied to the nonâ€‘identity branch, then summed with the identity connection. This integration enables dynamic channelâ€‘wise attention within residual architectures.

---

![Table 1](images/table1.jpg)  
*TableÂ 1:* Network configurations (block counts, channel dimensions) for ResNet and SEâ€‘ResNet variants, as reported in the original paper.  
 
> **Model highlights:**  
> - SE blocks **adaptively recalibrate channel-wise features**, boosting informative signals.  
> - Residual connections **preserve gradient flow** for stable deep network training.  
> - Global average pooling + flatten prepares features for the classifier.  
> - Homogeneous residual + SE topology ensures **scalability** across depths.

---

## ğŸ§® Key Mathematical Idea

![Math Concept](images/math.jpg)  

---

## ğŸ—ï¸ Model Architecture

```bash
SE-ResNet/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_layer.py             # Standard convolution
â”‚   â”‚   â”œâ”€â”€ se_block.py               # Squeeze-and-Excitation module
â”‚   â”‚   â”œâ”€â”€ flatten_layer.py          # Flatten layer
â”‚   â”‚   â”œâ”€â”€ fc_layer.py               # Fully connected classifier
â”‚   â”‚   â”œâ”€â”€ pool_layers/
â”‚   â”‚   â”‚   â”œâ”€â”€ maxpool_layer.py      # MaxPool
â”‚   â”‚   â”‚   â””â”€â”€ avgpool_layer.py      # Global/AdaptiveAvgPool for SE
â”‚   â”‚
â”‚   â”œâ”€â”€ blocks/
â”‚   â”‚   â””â”€â”€ residual_block.py         # Residual block + SE integration
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ se_resnet.py              # Full SE-ResNet: Stem + Residual+SE blocks + Classifier
â”‚   â”‚
â”‚   â””â”€â”€ config.py                      # Input size, num_classes, depth, reduction ratio
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ table1.jpg                     # Parameter/accuracy comparison
â”‚   â”œâ”€â”€ figures.jpg                    # Figures 1-3 from the paper
â”‚   â””â”€â”€ math.jpg                       # Key SE block equations
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
